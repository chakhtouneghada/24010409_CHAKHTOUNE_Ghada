1. Le Contexte MÃ©tier et la Mission
Le ProblÃ¨me (Business Case)
Dans le secteur bancaire, le volume Ã©levÃ© de transactions et la diversitÃ© des canaux (agences, distributeurs automatiques, services en ligne) rendent difficile la dÃ©tection manuelle des opÃ©rations anormales ou potentiellement frauduleuses.
Certaines transactions prÃ©sentent des montants inhabituels ou des caractÃ©ristiques atypiques (contexte, frÃ©quence, profil client) et peuvent reprÃ©senter un risque, mais restent noyÃ©es dans un flux massif dâ€™opÃ©rations.

Objectif : Mettre en place un pipeline de Machine Learning capable de distinguer des transactions Â« normales Â» de transactions Â« Ã  risque Â», en sâ€™appuyant sur les informations disponibles dans un fichier de transactions bancaires.
Lâ€™Enjeu critique : La matrice des coÃ»ts dâ€™erreur est asymÃ©trique.

Classer comme Â« Ã  risque Â» une transaction lÃ©gitime (Faux Positif) peut gÃ©nÃ©rer du stress pour le client, des vÃ©rifications manuelles et des coÃ»ts opÃ©rationnels.

Classer comme Â« normale Â» une transaction rÃ©ellement problÃ©matique (Faux NÃ©gatif) peut entraÃ®ner des pertes financiÃ¨res, une fraude non dÃ©tectÃ©e et des risques de nonâ€‘conformitÃ©.

Dans un contexte de dÃ©tection de risque, il est donc important de prioriser le rappel (Recall) sur la classe Â« Ã  risque Â», quitte Ã  accepter davantage de Faux Positifs.

Les DonnÃ©es (Lâ€™Input)
Nous utilisons un fichier de transactions bancaires : bank_transactions_data.csv.

X (Features) : ce sont les caractÃ©ristiques descriptives de chaque transaction, comprenant par exemple :

Identifiants techniques : TransactionID, AccountID

Variables financiÃ¨res : TransactionAmount, AccountBalance

Informations temporelles : TransactionDate, PreviousTransactionDate

Contexte de la transaction : Location, Channel, DeviceID, IP Address

Profil client : CustomerAge, CustomerOccupation, LoginAttempts, TransactionDuration

y (Target) : une cible binaire is_risky est construite de maniÃ¨re pÃ©dagogique.

0 = Transaction considÃ©rÃ©e Â« normale Â»

1 = Transaction considÃ©rÃ©e Â« Ã  risque potentielle Â» (par exemple les 5% de plus gros montants)

2. Le Code Python (Laboratoire)
Ce script est la paillasse de laboratoire. Il contient toutes les manipulations nÃ©cessaires : chargement des donnÃ©es, construction de la cible, nettoyage, analyse exploratoire, sÃ©paration Train/Test, entraÃ®nement dâ€™un modÃ¨le Random Forest et audit de ses performances.

python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Configuration
sns.set_theme(style="whitegrid")
import warnings
warnings.filterwarnings("ignore")

# --- PHASE 1 : ACQUISITION DES DONNÃ‰ES BANCAIRES ---
df = pd.read_csv("bank_transactions_data.csv")

print("=== APERÃ‡U DU DATASET ===")
print(f"Taille du dataset : {df.shape}")
print("Colonnes disponibles :")
print(df.columns.tolist())
print()

# Construction d'une cible binaire pÃ©dagogique : is_risky
threshold = df["TransactionAmount"].quantile(0.95)
df["is_risky"] = (df["TransactionAmount"] > threshold).astype(int)

print("Colonne cible crÃ©Ã©e : 'is_risky' (0 = normal, 1 = transaction Ã  risque potentielle)")
print(f"RÃ©partition de la cible :\n{df['is_risky'].value_counts(normalize=True)}\n")

# --- PHASE 2 : DATA WRANGLING (NETTOYAGE & PRÃ‰PARATION) ---
cols_to_drop = [
    "TransactionID",
    "AccountID",
    "TransactionDate",
    "PreviousTransactionDate",
    "IP Address"
]
df_model = df.drop(columns=[c for c in cols_to_drop if c in df.columns])

X = df_model.drop("is_risky", axis=1)
y = df_model["is_risky"]

# Encodage des variables catÃ©gorielles
X = pd.get_dummies(X, drop_first=True)

# Imputation des valeurs manquantes
imputer = SimpleImputer(strategy="mean")
X_imputed = imputer.fit_transform(X)
X_clean = pd.DataFrame(X_imputed, columns=X.columns)

print(f"Nombre total de valeurs manquantes restantes : {X_clean.isnull().sum().sum()}\n")

# --- PHASE 3 : ANALYSE EXPLORATOIRE (EDA) ---
print("--- Statistiques Descriptives (variables financiÃ¨res) ---")
num_cols = [c for c in X_clean.columns if "TransactionAmount" in c or "AccountBalance" in c]
if len(num_cols) > 0:
    print(X_clean[num_cols].describe())

plt.figure(figsize=(8, 4))
sns.histplot(df["TransactionAmount"], kde=True)
plt.title("Distribution du montant des transactions")
plt.xlabel("TransactionAmount")
plt.tight_layout()
plt.show()

# --- PHASE 4 : PROTOCOLE EXPÃ‰RIMENTAL (SPLIT) ---
X_train, X_test, y_train, y_test = train_test_split(
    X_clean,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"EntraÃ®nement : {X_train.shape[0]} Ã©chantillons")
print(f"Test        : {X_test.shape[0]} Ã©chantillons\n")

# --- PHASE 5 : INTELLIGENCE ARTIFICIELLE (RANDOM FOREST) ---
model = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    n_jobs=-1,
    class_weight="balanced"
)
model.fit(X_train, y_train)

# --- PHASE 6 : AUDIT DE PERFORMANCE ---
y_pred = model.predict(X_test)

print(f"\n--- Accuracy Globale : {accuracy_score(y_test, y_pred)*100:.2f}% ---")
print("\n--- Rapport DÃ©taillÃ© ---")
print(classification_report(y_test, y_pred))

plt.figure(figsize=(6, 5))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title('Matrice de Confusion : Transactions normales vs Ã  risque')
plt.ylabel('Vraie Classe')
plt.xlabel('Classe PrÃ©dite')
plt.tight_layout()
plt.show()
3. Analyse Approfondie : Nettoyage (Data Wrangling)
Le ProblÃ¨me MathÃ©matique du Â« Vide Â»
Les algorithmes de Machine Learning reposent sur des opÃ©rations dâ€™algÃ¨bre linÃ©aire qui ne supportent pas la prÃ©sence de valeurs NaN.
Une seule valeur manquante dans une colonne peut suffire Ã  faire Ã©chouer un entraÃ®nement ou une prÃ©diction.

Dans un jeu de donnÃ©es de transactions bancaires, ces valeurs manquantes peuvent provenir dâ€™informations client incomplÃ¨tes, de champs facultatifs non renseignÃ©s ou dâ€™erreurs dâ€™enregistrement.
Il est donc indispensable de remplacer ou de traiter ces Â« vides Â» avant de passer Ã  la phase de modÃ©lisation.

La MÃ©canique de lâ€™Imputation
Nous utilisons SimpleImputer(strategy="mean").

Apprentissage (fit) : lâ€™imputer parcourt chaque colonne numÃ©rique de X et calcule la moyenne des valeurs disponibles. Il mÃ©morise cette moyenne pour chaque feature (par exemple, le montant moyen ou le solde moyen).

Transformation (transform) : lors de la transformation, toutes les valeurs manquantes dâ€™une colonne sont remplacÃ©es par la moyenne apprise.

Cette stratÃ©gie produit un tableau X_clean sans NaN, prÃªt Ã  Ãªtre utilisÃ© par le modÃ¨le.

ğŸ’¡ Coin de lâ€™Expert (Data Leakage)
Dans un projet rigoureux, il faut Ã©viter que lâ€™information du jeu de test se retrouve injectÃ©e dans les statistiques utilisÃ©es pour le nettoyage.
La bonne pratique consiste Ã  ajuster lâ€™imputer uniquement sur le jeu dâ€™entraÃ®nement, puis Ã  appliquer cette transformation au jeu de test.

4. Analyse Approfondie : Exploration (EDA)
Câ€™est lâ€™Ã©tape de Â« profilage Â» des transactions.

DÃ©crypter .describe()
Lâ€™appel Ã  describe() sur des variables comme TransactionAmount ou AccountBalance fournit plusieurs informations clÃ©s :

Mean (Moyenne) vs 50% (MÃ©diane) : si la moyenne est nettement plus Ã©levÃ©e que la mÃ©diane, cela indique une distribution asymÃ©trique, tirÃ©e par quelques transactions de trÃ¨s gros montant.

Std (Ã‰cart-type) : mesure la dispersion des valeurs. Un Ã©cart-type Ã©levÃ© signifie que les montants sont trÃ¨s variÃ©s, ce qui peut rendre le problÃ¨me plus complexe pour le modÃ¨le.

La MulticollinÃ©aritÃ© (Le problÃ¨me de la redondance)
En Ã©tudiant une matrice de corrÃ©lation sur les variables numÃ©riques (montant, solde, durÃ©e, etc.), certaines paires de colonnes peuvent apparaÃ®tre fortement corrÃ©lÃ©es.

Sur le plan Ã©conomique, cela peut Ãªtre logique : un solde courant peut Ãªtre liÃ© Ã  un solde moyen ou Ã  la frÃ©quence des transactions.

Pour un Random Forest, cette redondance pose peu de problÃ¨mes car les arbres sÃ©lectionnent des sousâ€‘ensembles de variables et gÃ¨rent bien les corrÃ©lations. Pour des modÃ¨les linÃ©aires, une forte multicolinÃ©aritÃ© peut rendre les coefficients difficiles Ã  interprÃ©ter et instables.

5. Analyse Approfondie : MÃ©thodologie (Split)
Le Concept : La Garantie de GÃ©nÃ©ralisation
Le but du Machine Learning nâ€™est pas de mÃ©moriser les transactions passÃ©es, mais de gÃ©nÃ©raliser Ã  de nouvelles opÃ©rations.
SÃ©parer les donnÃ©es en deux ensembles â€“ un pour lâ€™entraÃ®nement, un pour le test â€“ permet de vÃ©rifier la capacitÃ© rÃ©elle du modÃ¨le Ã  se comporter correctement sur des donnÃ©es jamais vues.

Les ParamÃ¨tres sous le capot
La sÃ©paration utilisÃ©e est :

test_size=0.2 : environ 80% des transactions sont utilisÃ©es pour lâ€™entraÃ®nement, 20% pour le test.

random_state=42 : la graine fixe le tirage alÃ©atoire, ce qui garantit la reproductibilitÃ© des rÃ©sultats.

stratify=y : le ratio entre transactions normales et Ã  risque est conservÃ© dans les deux sousâ€‘ensembles.

Le ratio 80/20 permet au modÃ¨le de disposer dâ€™assez dâ€™exemples pour apprendre des schÃ©mas robustes tout en gardant suffisamment de donnÃ©es pour Ã©valuer la performance finale de maniÃ¨re fiable.
La reproductibilitÃ© est essentielle pour pouvoir comparer plusieurs versions du modÃ¨le dans le temps.

6. FOCUS THÃ‰ORIQUE : Lâ€™Algorithme Random Forest ğŸŒ²
A. La Faiblesse de lâ€™Individu (Arbre de DÃ©cision)
Un arbre de dÃ©cision unique pose des questions successives sur les variables (montant, Ã¢ge du client, canal, localisation, etc.) pour aboutir Ã  une prÃ©diction.
Le problÃ¨me est quâ€™il peut facilement surâ€‘apprendre : si une transaction trÃ¨s atypique apparaÃ®t, lâ€™arbre peut crÃ©er une rÃ¨gle trÃ¨s spÃ©cifique juste pour ce cas, ce qui conduit Ã  une forte variance.

B. La Force du Groupe (Bagging)
Random Forest signifie Â« ForÃªt AlÃ©atoire Â». Le modÃ¨le construit de nombreux arbres Ã  partir de variations des donnÃ©es et des variables.

Bootstrapping (DiversitÃ© des Ã‰chantillons) : chaque arbre est entraÃ®nÃ© sur un Ã©chantillon tirÃ© avec remise Ã  partir des donnÃ©es dâ€™entraÃ®nement. Chaque arbre voit donc une version lÃ©gÃ¨rement diffÃ©rente de lâ€™historique de transactions.

Feature Randomness (DiversitÃ© des Questions) : Ã  chaque nÅ“ud, un arbre ne considÃ¨re quâ€™un sousâ€‘ensemble alÃ©atoire de variables pour dÃ©cider du meilleur split. Cela oblige les arbres Ã  explorer des combinaisons de features moins Ã©videntes et Ã©vite quâ€™ils ne se focalisent tous sur la mÃªme variable (par exemple, uniquement le montant).

C. Le Consensus (Vote)
Pour une nouvelle transaction, tous les arbres de la forÃªt produisent une prÃ©diction (normale ou Ã  risque).
La classe finale est dÃ©terminÃ©e par un vote majoritaire. Les erreurs individuelles de certains arbres se compensent, ce qui renforce la stabilitÃ© du modÃ¨le et la qualitÃ© des prÃ©dictions sur des donnÃ©es bruitÃ©es et variÃ©es comme les flux bancaires.

7. Analyse Approfondie : Ã‰valuation (Lâ€™Heure de VÃ©ritÃ©)
A. La Matrice de Confusion (Quadrants)
La matrice de confusion synthÃ©tise les performances de la maniÃ¨re suivante :

Vrais Positifs (TP) : transactions Ã  risque correctement dÃ©tectÃ©es comme Ã  risque.

Vrais NÃ©gatifs (TN) : transactions normales correctement classÃ©es comme normales.

Faux Positifs (FP) : transactions normales classÃ©es par erreur comme Ã  risque.

Faux NÃ©gatifs (FN) : transactions Ã  risque classÃ©es par erreur comme normales.

Dans un systÃ¨me de dÃ©tection de risque, les Faux NÃ©gatifs sont particuliÃ¨rement critiques, car ils correspondent Ã  des opÃ©rations problÃ©matiques non repÃ©rÃ©es.
Les Faux Positifs restent nÃ©anmoins importants Ã  surveiller pour ne pas dÃ©grader lâ€™expÃ©rience client.

B. Les MÃ©triques AvancÃ©es
Lâ€™accuracy seule peut Ãªtre trompeuse lorsque la classe Ã  risque est rare. Il est donc nÃ©cessaire de regarder :

PrÃ©cision (Precision) : mesure la proportion de transactions rÃ©ellement Ã  risque parmi celles que le modÃ¨le a signalÃ©es. Une prÃ©cision faible signifie trop de fausses alertes.

Rappel (Recall / SensibilitÃ©) : mesure la proportion de transactions Ã  risque correctement dÃ©tectÃ©es parmi toutes les transactions Ã  risque prÃ©sentes dans les donnÃ©es. Un rappel faible signifie que le modÃ¨le laisse passer trop dâ€™opÃ©rations dangereuses.

F1-Score : combine prÃ©cision et rappel en une seule mÃ©trique. Il est particuliÃ¨rement utile pour comparer des modÃ¨les lorsquâ€™il existe un dÃ©sÃ©quilibre de classes.

Conclusion du Projet
Ce projet montre que la Data Science appliquÃ©e aux transactions bancaires ne se limite pas Ã  lâ€™entraÃ®nement dâ€™un modÃ¨le.
Il sâ€™agit dâ€™une chaÃ®ne cohÃ©rente de dÃ©cisions : comprÃ©hension du contexte mÃ©tier, prÃ©paration minutieuse des donnÃ©es, analyse exploratoire, dÃ©finition dâ€™un protocole expÃ©rimental robuste, choix dâ€™un algorithme adaptÃ© (Random Forest) et interprÃ©tation rigoureuse des mÃ©triques dâ€™Ã©valuation.
